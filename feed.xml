<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://ertyu116-116.github.io/yongyunsong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ertyu116-116.github.io/yongyunsong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-01-14T00:42:17+00:00</updated><id>https://ertyu116-116.github.io/yongyunsong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Streo Vision</title><link href="https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/Stereo-Vision/" rel="alternate" type="text/html" title="Streo Vision"/><published>2022-01-14T00:00:00+00:00</published><updated>2022-01-14T00:00:00+00:00</updated><id>https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/Stereo-Vision</id><content type="html" xml:base="https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/Stereo-Vision/"><![CDATA[<ul> <li>3D reconstruction <ul> <li>Shading, focus/defocus, texture, perspective effects, motion, occlusion, stereo</li> </ul> </li> <li>Human stereopsis <ul> <li>fixation, disparity</li> </ul> </li> <li>Multiple views <ul> <li>epipolar geometry</li> </ul> </li> <li>Stereo Vision <ul> <li>geometry of a parallel stereo system</li> <li>correspondence</li> </ul> </li> </ul> <h2 id="methods-of-3d-reconstruction">Methods of 3D reconstruction</h2> <ul> <li> <h3 id="dimensionality-reduction">Dimensionality Reduction</h3> <p>The spatial information is lost while 3D object is projected to 2D. In that case, lengths are lost and angle preservation is lost. Hence, Parallel line are lost but there is a vanishing point where two lines, which was once parallel line, meet together.</p> <p>From this vanishing point, we can find out and track the spatial information lost.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Dimensinality_Reduction-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Dimensinality_Reduction-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Dimensinality_Reduction-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Dimensinality_Reduction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure1. Dimensinality reduction </div> <ul> <li> <h3 id="structure-from-shadow">Structure from Shadow</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Shading-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Shading-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Shading-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Shading.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure2. Structure from Shadow </div> <ul> <li>Lmertian Reflectance Model for approximating the reflectance property of the diffuse surface</li> </ul> \[Diffuse\; Surface\; Color = \frac{\rho_d}{\pi} *L_i * cos{\theta}\] <p>Light from all points on the surface reaches the viewer (camera). It means we can deduce the 3D surface from intensity and angles.</p> <ul> <li> <h3 id="structure-from-focus">Structure from focus</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Focus-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Focus-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Focus-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Focus.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure3. Structure from focus </div> <p>Despite the same images, we can deduce 3D information while using low-pass filter with different sigma to make images blur or not.</p> <ul> <li> <h3 id="texture">Texture</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Texture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Texture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Texture-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Shape_from_Texture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure4. Structure form Texture </div> <p>We can infer the 3D data while measuring images pattern. e.g. If the white seeds of strawberry is close, it means the distance is close. If the white seeds of strawberry is far, it means the distance is far.</p> <ul> <li> <h3 id="perspective-effects">Perspective effects</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Vanishing_point-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Vanishing_point-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Vanishing_point-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Vanishing_point.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure5. Vanishing point </div> <p>Perspective effect means even paraller line converge on one point while 3D image is prjected to 2D image. We can infer spatial information from vanishing point. the side effect of this method,However, is there is foriegn objects of air and the signal is hard to reach on camera. e.g.) when we take a mountain photo, the more the object is far from the camera, the more ambiguous the object is.</p> <ul> <li> <h3 id="stereo">Stereo</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Stereo_Vision-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Stereo_Vision-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Stereo_Vision-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Stereo_Vision.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure6. Stereo Vision </div> <p>It is important to note that the camera centor should be moved not fixed with rotation in Stereo Vision for obtaining exact 3D information.</p> <h2 id="disparity">Disparity</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Disparity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Disparity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Disparity-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Disparity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure7. Human steropsis: disparity </div> <p>disparity occurs when eyes fixate on one object; others appear at different visual angles. Disparity is distance from b1 to b2 along retina.</p> <h2 id="multiple-views">Multiple views</h2> <p>Why we have multiple views? Because there is distortion when projection occurs. e.g. the line which is connected the optical center of camera is projected to one point, not line.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Multiple_views-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Multiple_views-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Multiple_views-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Multiple_views.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure8. Multiple views </div> <p>In multiple views, we shift the projection image to infront of the optical center than using pin hole model which change the locations for 2D coordinate,as an ideal mathematic method.</p> <h2 id="stereo-vision">Stereo Vision</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Epipolar_Geometry-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Epipolar_Geometry-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Stereo%20Vision/Epipolar_Geometry-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Stereo%20Vision/Epipolar_Geometry.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure9. Epipolar Geometry </div> <p>epipole - when the line which connects optical centers of two cameras, e and e’ are called to epipoles. epipolar line - the line which connects epipole and point projected. epipolar constraint - Depending on DOF(Degree of Freedom), there are the 8,7,5,3 and 1 pair of matching points to obtain fundamental and essential matrix. Triangulation - If we know E, F matrix and p,p’, we can obtain 3D point.</p> <h2 id="reference">Reference</h2> <ul> <li>https://www.youtube.com/watch?v=3QjEOlfvg9M&amp;t=2326</li> <li>https://math.hws.edu/graphicsbook/c4/s1.html</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Characteristics of Stereo Vision]]></summary></entry><entry><title type="html">Feature Detection</title><link href="https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/feature-detection-copy/" rel="alternate" type="text/html" title="Feature Detection"/><published>2022-01-05T00:00:00+00:00</published><updated>2022-01-05T00:00:00+00:00</updated><id>https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/feature-detection%20copy</id><content type="html" xml:base="https://ertyu116-116.github.io/yongyunsong.github.io/blog/2022/feature-detection-copy/"><![CDATA[<h2 id="what-is-a-good-feature">What is a good feature?</h2> <ul> <li>Repeatability <ul> <li>The same feature can be found in several images despite geometric and photometric transformations</li> </ul> </li> <li>Saliency <ul> <li>Each feature is distinctive.</li> </ul> </li> <li>Compactness and Efficiency <ul> <li>Many fewer features than image pixels</li> </ul> </li> <li>Locality <ul> <li>A feature occupies a relatively small area of the image; robust to clutter and occlusion.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Corner-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Corner-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Corner-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Corner.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure1. Auto-correlation </div> <h3 id="corner-is-good-as-a-feature">Corner is good as a feature</h3> <ul> <li>There is no change in a flat area, i.e., cloud</li> <li>There is no vertical/horizontal change in a edge, i.e., vertical/horizontal line</li> </ul> <p>Therefore, we have to find out corner of image.</p> <h2 id="what-is-euv-formular">What is E(u,v) formular?</h2> \[E(u,v) = \sum_{x,y}w(x,y)[I(x+u,y+v) - I(x,y)]^2\] <p>Corner Detection by Auto-correlation</p> <p>w(x,y) : we can call it as a filter, window and mask. i.e., gaussian filter, unit step function. I(x+u,y+v) - I(x,y): the difference between the pixel value of the original image and the pixel brightness of the image moved by u+v</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Auto-correlation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Auto-correlation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Auto-correlation-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Auto-correlation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure2. Auto-correlation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Correspondency-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Correspondency-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Correspondency-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Correspondency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure3. (a) : Varied a lot of changes of pixel intensity (b): Edge (c): No change </div> <p>There is a lot of computational cost:</p> \[O(window\_width^2 * shift\_range^2 * image\_width^2)\] <p>ex) \(O(11^2 * 11^2 * 600^2) = 5.2 billion\)</p> <p>Can we just approximate E(u,v) locally by a quadratic surface? Yes, if we use taylor series expansion</p> <h2 id="explanation-of-talyor-expansion">Explanation Of Talyor expansion</h2> <ul> <li>Talyor Series to quadratic differential</li> </ul> \[f(x,y) \approx f(a,b) + f_{x}(a,b)(x-a) + f_{y}(a,b)(y-b) + \frac{1}{2!}(f_{xx}(a,b)(x-a)^2 + f_{xy}(a,b)(x-a)(x-b)+ f_{yy}(a,b)(y-b)^2)\] <h2 id="application-of-talyor-expansion">Application Of Talyor expansion</h2> <p>Local quadratic approximation of E(u,v) in the neighborhood of (0,0) is given by the second-order Taylor expansion:</p> <ul> <li> \[E(u,v) \approx E(0,0) + \begin{bmatrix}u &amp; v\end{bmatrix}\begin{bmatrix}E_{u}(0,0)\\ E_{v}(0,0) \end{bmatrix}+ \frac{1}{2}\begin{bmatrix}u &amp; v\end{bmatrix}\begin{bmatrix}E_{uu}(0,0) &amp; E_{uv}(0,0)\\ E_{uv}(0,0)&amp; E_{vv}(0,0)\end{bmatrix}\begin{bmatrix}u\\ v \end{bmatrix}\] </li> <li> \[E_{u}(u,v) = \sum_{x,y}2w(x,y)[I(x+u,y+v)-I(x,y)]I_{x}(x+u,y+v)\] </li> <li> \[E_{uu}(u,v) = \sum_{x,y}2w(x,y)I_{x}(x+u,y+v)I_{x}(x+u,y+v) + \sum_{x,y}2w(x,y)[I(x+u,y+v)-I(x,y)]I_{xx}(x+u,y+v)\] </li> <li> \[E_{uv}(u,v) = \sum_{x,y}2w(x,y)I_{y}(x+u,y+v)I_{x}(x+u,y+v) + \sum_{x,y}2w(x,y)[I(x+u,y+v)-I(x,y)]I_{xy}(x+u,y+v)\] <ul> <li> <p>Let u,v equals 0: it means \(I(x+u,y+v)-I(x,y) = 0\)</p> \[E_{u}(0,0) = 0\] \[E_{v}(0,0) = 0\] \[E_{uu}(u,v) = \sum_{x,y}2w(x,y)I_{x}(x,y)I_{x}(x,y)\] \[E_{vv}(u,v) = \sum_{x,y}2w(x,y)I_{y}(x,y)I_{y}(x,y)\] \[E_{uv}(u,v) = \sum_{x,y}2w(x,y)I_{x}(x,y)I_{y}(x,y)\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Moment%20Matrix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Moment%20Matrix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Moment%20Matrix-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Moment%20Matrix.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure4. Moment Matrix means Coefficient of Quadratic function </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Cornerness-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Cornerness-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Cornerness-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Cornerness.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure5. Cornerness determines which feature is flat, edge or corner. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Visualization%20of%20second%20moment%20matrices-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Visualization%20of%20second%20moment%20matrices-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/yongyunsong.github.io/assets/img/Feature%20Detection/Visualization%20of%20second%20moment%20matrices-1400.webp"/> <img src="/yongyunsong.github.io/assets/img/Feature%20Detection/Visualization%20of%20second%20moment%20matrices.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure6. Visualization of second moment matrices </div> <h2 id="reference">Reference</h2> <ul> <li>https://youtu.be/v1cdAgkCHqE</li> <li>https://www.youtube.com/watch?v=3d6DsjIBzJ4</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Characteristics of good features]]></summary></entry></feed>